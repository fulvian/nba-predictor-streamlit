#!/usr/bin/env python3
"""
ğŸ” MODEL VALIDATION AUDIT SCRIPT
Analizza le performance del modello probabilistico esistente
"""

import pandas as pd
import numpy as np
import joblib
from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns

def audit_model_performance():
    """Esegue audit completo delle performance del modello"""
    
    print("ğŸ” === NBA PREDICTOR MODEL AUDIT ===")
    
    # 1. CARICA DATASET TRAINING
    try:
        print("\nğŸ“Š Loading dataset...")
        df = pd.read_csv('data/nba_data_with_mu_sigma_for_ml.csv')
        print(f"   âœ… Dataset loaded: {len(df)} samples, {len(df.columns)} features")
        
        # Analisi dataset
        print(f"\nğŸ“ˆ DATASET ANALYSIS:")
        print(f"   ğŸ“… Date range: {df['GAME_DATE_EST'].min()} to {df['GAME_DATE_EST'].max()}")
        print(f"   ğŸ€ Seasons: {df['SEASON'].unique()}")
        print(f"   ğŸ“Š Target stats:")
        print(f"      MU_L1_Media_punti_stimati_finale: {df['MU_L1_Media_punti_stimati_finale'].mean():.1f} Â± {df['MU_L1_Media_punti_stimati_finale'].std():.1f}")
        print(f"      SIGMA_L2_sd_final: {df['SIGMA_L2_sd_final'].mean():.1f} Â± {df['SIGMA_L2_sd_final'].std():.1f}")
        
    except Exception as e:
        print(f"   âŒ Error loading dataset: {e}")
        return
    
    # 2. CARICA MODELLI
    try:
        print("\nğŸ¤– Loading models...")
        mu_model = joblib.load('models/probabilistic/mu_model.pkl')
        sigma_model = joblib.load('models/probabilistic/sigma_model.pkl')
        scaler = joblib.load('models/probabilistic/scaler.pkl')
        print(f"   âœ… Models loaded:")
        print(f"      Mu Model: {type(mu_model).__name__}")
        print(f"      Sigma Model: {type(sigma_model).__name__}")
        print(f"      Scaler: {type(scaler).__name__}")
        
    except Exception as e:
        print(f"   âŒ Error loading models: {e}")
        return
    
    # 3. PREPARA FEATURES (replico la logica del probabilistic_model.py)
    try:
        print("\nâš™ï¸ Preparing features...")
        
        # Features originali (basate sul codice probabilistic_model.py)
        feature_columns = [
            'HOME_ORtg_sAvg', 'HOME_DRtg_sAvg', 'HOME_PACE_season',
            'AWAY_ORtg_sAvg', 'AWAY_DRtg_sAvg', 'AWAY_PACE_season', 
            'HOME_eFG_PCT_sAvg', 'HOME_TOV_PCT_sAvg', 'HOME_OREB_PCT_sAvg', 'HOME_FT_RATE_sAvg',
            'AWAY_eFG_PCT_sAvg', 'AWAY_TOV_PCT_sAvg', 'AWAY_OREB_PCT_sAvg', 'AWAY_FT_RATE_sAvg',
            'HOME_ORtg_L5Avg', 'HOME_DRtg_L5Avg',
            'AWAY_ORtg_L5Avg', 'AWAY_DRtg_L5Avg',
            'GAME_PACE'
        ]
        
        # Trova colonne disponibili nel dataset
        available_features = []
        for col in feature_columns:
            if col in df.columns:
                available_features.append(col)
            else:
                # Cerca alternative
                alternatives = [c for c in df.columns if col.replace('_season', '').replace('_sAvg', '') in c]
                if alternatives:
                    print(f"   âš ï¸ {col} not found, using {alternatives[0]}")
                    available_features.append(alternatives[0])
                else:
                    print(f"   âŒ {col} not found in dataset")
        
        print(f"   âœ… Using {len(available_features)} features")
        
        # Estrai features e targets
        X = df[available_features].fillna(method='ffill').fillna(110.0)  # Default NBA values
        y_mu = df['MU_L1_Media_punti_stimati_finale'].fillna(220.0)
        y_sigma = df['SIGMA_L2_sd_final'].fillna(12.0)
        
        print(f"   ğŸ“Š Feature matrix: {X.shape}")
        print(f"   ğŸ¯ Targets: Î¼={len(y_mu)}, Ïƒ={len(y_sigma)}")
        
    except Exception as e:
        print(f"   âŒ Error preparing features: {e}")
        return
    
    # 4. SPLIT TEMPORALE (piÃ¹ realistico per time series)
    try:
        print("\nğŸ“… Temporal train/test split...")
        
        df['GAME_DATE'] = pd.to_datetime(df['GAME_DATE_EST'])
        df_sorted = df.sort_values('GAME_DATE')
        
        # 80/20 split temporale
        split_idx = int(len(df_sorted) * 0.8)
        
        train_idx = df_sorted.index[:split_idx]
        test_idx = df_sorted.index[split_idx:]
        
        X_train, X_test = X.loc[train_idx], X.loc[test_idx]
        y_mu_train, y_mu_test = y_mu.loc[train_idx], y_mu.loc[test_idx]
        y_sigma_train, y_sigma_test = y_sigma.loc[train_idx], y_sigma.loc[test_idx]
        
        print(f"   âœ… Train: {len(X_train)} samples")
        print(f"   âœ… Test: {len(X_test)} samples")
        print(f"   ğŸ“… Train period: {df_sorted.loc[train_idx, 'GAME_DATE'].min()} to {df_sorted.loc[train_idx, 'GAME_DATE'].max()}")
        print(f"   ğŸ“… Test period: {df_sorted.loc[test_idx, 'GAME_DATE'].min()} to {df_sorted.loc[test_idx, 'GAME_DATE'].max()}")
        
    except Exception as e:
        print(f"   âŒ Error in train/test split: {e}")
        return
    
    # 5. SCALE FEATURES
    try:
        print("\nâš–ï¸ Scaling features...")
        X_train_scaled = scaler.transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        print(f"   âœ… Features scaled using existing scaler")
        
    except Exception as e:
        print(f"   âŒ Error scaling features: {e}")
        return
    
    # 6. EVALUATE MODELS
    try:
        print("\nğŸ“Š === MODEL EVALUATION ===")
        
        # Predizioni Mu
        print("\nğŸ¯ MU MODEL PERFORMANCE:")
        mu_pred_train = mu_model.predict(X_train_scaled)
        mu_pred_test = mu_model.predict(X_test_scaled)
        
        mu_mae_train = mean_absolute_error(y_mu_train, mu_pred_train)
        mu_mae_test = mean_absolute_error(y_mu_test, mu_pred_test)
        mu_r2_train = r2_score(y_mu_train, mu_pred_train)
        mu_r2_test = r2_score(y_mu_test, mu_pred_test)
        mu_rmse_train = np.sqrt(mean_squared_error(y_mu_train, mu_pred_train))
        mu_rmse_test = np.sqrt(mean_squared_error(y_mu_test, mu_pred_test))
        
        print(f"   ğŸ‹ï¸ TRAIN: MAE={mu_mae_train:.2f}, RMSE={mu_rmse_train:.2f}, RÂ²={mu_r2_train:.3f}")
        print(f"   ğŸ§ª TEST:  MAE={mu_mae_test:.2f}, RMSE={mu_rmse_test:.2f}, RÂ²={mu_r2_test:.3f}")
        
        # Predizioni Sigma
        print("\nğŸ“ SIGMA MODEL PERFORMANCE:")
        sigma_pred_train = sigma_model.predict(X_train_scaled)
        sigma_pred_test = sigma_model.predict(X_test_scaled)
        
        sigma_mae_train = mean_absolute_error(y_sigma_train, sigma_pred_train)
        sigma_mae_test = mean_absolute_error(y_sigma_test, sigma_pred_test)
        sigma_r2_train = r2_score(y_sigma_train, sigma_pred_train)
        sigma_r2_test = r2_score(y_sigma_test, sigma_pred_test)
        sigma_rmse_train = np.sqrt(mean_squared_error(y_sigma_train, sigma_pred_train))
        sigma_rmse_test = np.sqrt(mean_squared_error(y_sigma_test, sigma_pred_test))
        
        print(f"   ğŸ‹ï¸ TRAIN: MAE={sigma_mae_train:.2f}, RMSE={sigma_rmse_train:.2f}, RÂ²={sigma_r2_train:.3f}")
        print(f"   ğŸ§ª TEST:  MAE={sigma_mae_test:.2f}, RMSE={sigma_rmse_test:.2f}, RÂ²={sigma_r2_test:.3f}")
        
        # Overfitting check
        print(f"\nğŸ” OVERFITTING ANALYSIS:")
        mu_overfit = (mu_mae_test - mu_mae_train) / mu_mae_train * 100
        sigma_overfit = (sigma_mae_test - sigma_mae_train) / sigma_mae_train * 100
        
        print(f"   ğŸ¯ Mu overfitting: {mu_overfit:+.1f}% ({'âš ï¸ HIGH' if abs(mu_overfit) > 15 else 'âœ… OK'})")
        print(f"   ğŸ“ Sigma overfitting: {sigma_overfit:+.1f}% ({'âš ï¸ HIGH' if abs(sigma_overfit) > 15 else 'âœ… OK'})")
        
    except Exception as e:
        print(f"   âŒ Error in model evaluation: {e}")
        return
    
    # 7. RESIDUAL ANALYSIS
    try:
        print(f"\nğŸ”¬ RESIDUAL ANALYSIS:")
        
        mu_residuals = y_mu_test - mu_pred_test
        sigma_residuals = y_sigma_test - sigma_pred_test
        
        # Statistiche residui
        print(f"   ğŸ¯ Mu residuals: mean={mu_residuals.mean():.2f}, std={mu_residuals.std():.2f}")
        print(f"   ğŸ“ Sigma residuals: mean={sigma_residuals.mean():.2f}, std={sigma_residuals.std():.2f}")
        
        # Test normalitÃ 
        mu_shapiro = stats.shapiro(mu_residuals.sample(min(5000, len(mu_residuals))))
        sigma_shapiro = stats.shapiro(sigma_residuals.sample(min(5000, len(sigma_residuals))))
        
        print(f"   ğŸ“Š Mu normality test: p={mu_shapiro.pvalue:.4f} ({'âœ… Normal' if mu_shapiro.pvalue > 0.05 else 'âš ï¸ Non-normal'})")
        print(f"   ğŸ“Š Sigma normality test: p={sigma_shapiro.pvalue:.4f} ({'âœ… Normal' if sigma_shapiro.pvalue > 0.05 else 'âš ï¸ Non-normal'})")
        
    except Exception as e:
        print(f"   âŒ Error in residual analysis: {e}")
    
    # 8. SUMMARY & RECOMMENDATIONS
    print(f"\nğŸ¯ === AUDIT SUMMARY ===")
    print(f"âœ… Model Type: XGBoost (good choice)")
    print(f"âœ… Dataset Size: {len(df)} samples (adequate)")
    print(f"âš ï¸ Test MAE Mu: {mu_mae_test:.1f} points ({'âœ… Good' if mu_mae_test < 8 else 'âš ï¸ Needs improvement' if mu_mae_test < 12 else 'âŒ Poor'})")
    print(f"âš ï¸ Test MAE Sigma: {sigma_mae_test:.1f} points ({'âœ… Good' if sigma_mae_test < 3 else 'âš ï¸ Needs improvement' if sigma_mae_test < 5 else 'âŒ Poor'})")
    
    recommendations = []
    if mu_mae_test > 10:
        recommendations.append("ğŸ”§ Improve Mu model: try feature engineering")
    if sigma_mae_test > 4:
        recommendations.append("ğŸ”§ Improve Sigma model: may need different approach")
    if abs(mu_overfit) > 15:
        recommendations.append("ğŸ”§ Reduce overfitting: regularization, early stopping")
    if len(available_features) < len(feature_columns):
        recommendations.append("ğŸ”§ Feature alignment: some features missing from dataset")
    
    if recommendations:
        print(f"\nğŸ“‹ RECOMMENDATIONS:")
        for rec in recommendations:
            print(f"   {rec}")
    else:
        print(f"\nğŸ‰ Model performance is acceptable!")

if __name__ == "__main__":
    audit_model_performance() 